{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd8b043",
   "metadata": {},
   "source": [
    "# Wildfire Fire Detection from Satellite Imagery "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce63713",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imported Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b0614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6908aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the functions directory\n",
    "sys.path.append('../functions')  # Add the path to the functions directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfe492",
   "metadata": {},
   "source": [
    "## 1. Load Dataset (Satellite Imagery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ccd57",
   "metadata": {},
   "source": [
    "### A. Image path specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d233a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import user defined function for loading image from /functions \n",
    "from img_read import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e6e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Image path\n",
    "imagePath = 'images/fire.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1eee2b",
   "metadata": {},
   "source": [
    "### B. Load image using loadImg(imagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0d9787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(imagePath)\n",
      "File \u001b[0;32m~/Documents/Fire-detection/model/../functions/img_read.py:14\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(imagePath)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image\u001b[39m(imagePath):\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Loads an image from a path, plots it and returns a NumPy array.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        np.ndarray: The loaded image as a NumPy array.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(imagePath)\n\u001b[1;32m     17\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "image = load_image(imagePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978f941-fbaf-42a9-92a1-3d5c926e2534",
   "metadata": {},
   "source": [
    "### C. Image dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580a2cc-99d8-4c05-b01f-1b4b97636e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image shape\n",
    "print(f\"Image shape : {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53efada",
   "metadata": {},
   "source": [
    "### D. View Image matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4b7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize orignal Image matrix\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df650c",
   "metadata": {},
   "source": [
    "## 2. Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d844b",
   "metadata": {},
   "source": [
    "### A. Normalize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c847f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalization\n",
    "print(\"Original Image Shape:\", image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "normalized_image = normalize_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalization\n",
    "print(\"Normalized Image Shape:\", normalized_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_image.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2d29d",
   "metadata": {},
   "source": [
    "### B. Plot Normalized Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704db74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min and Max Values after Normalization:\", np.min(normalized_image), np.max(normalized_image))\n",
    "plt.imshow(normalized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbabda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming normalized_image is a NumPy array in RGB format\n",
    "mpimg.imsave(\"images/normalized_fire_false_color_image.jpg\", normalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32463f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(normalized_image), np.max(normalized_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632aa96",
   "metadata": {},
   "source": [
    "### C. Normalized Image Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809d620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize normalized Image matrix\n",
    "print(normalized_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484d6ec",
   "metadata": {},
   "source": [
    "### D. Plot Original Image vs Normalized Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b6ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Visualize original vs normalized image\n",
    "\n",
    "# Display the images using Matplotlib\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the original image\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Original Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Plot the normalized image\n",
    "normalized_image_copy = normalized_image\n",
    "ax2.imshow(normalized_image_copy)\n",
    "ax2.set_title('Normalized Image')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c7093",
   "metadata": {},
   "source": [
    "### 2. Anotate the Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f9f10",
   "metadata": {},
   "source": [
    "### A.  Overlay anotation on normalized image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031b121",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import json\n",
    "\n",
    "# Load your image\n",
    "image = normalized_image.copy()\n",
    "\n",
    "# Load your annotation data\n",
    "with open(\"annotations/anot_norm.json\", \"r\") as file:\n",
    "    annotation_data = json.load(file)\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(image)\n",
    "\n",
    "# Mapping colors to classes\n",
    "class_colors = {\"Fire\": 'red', \"Smoke\": 'purple', \"No-Fire\": 'green'}\n",
    "\n",
    "# Overlay each bounding box on the image\n",
    "for box in annotation_data[\"boxes\"]:\n",
    "    if box[\"type\"] == \"polygon\":\n",
    "        # Extract polygon points\n",
    "        polygon_points = box[\"points\"]\n",
    "\n",
    "        # Determine class label\n",
    "        class_label = box[\"label\"]\n",
    "\n",
    "        # Define colors based on class\n",
    "        if class_label == \"1\":\n",
    "            edge_color, face_color = 'red', 'none'\n",
    "        elif class_label == \"2\":\n",
    "            edge_color, face_color = 'purple', 'none'\n",
    "        elif class_label == \"3\":\n",
    "            edge_color, face_color = 'green', 'none'\n",
    " \n",
    "\n",
    "        # Create a polygon patch\n",
    "        polygon = patches.Polygon(polygon_points, linewidth=0.7, edgecolor= edge_color, facecolor=face_color)\n",
    "\n",
    "        # Add the polygon to the axis\n",
    "        ax.add_patch(polygon)\n",
    "# Show legend for classes\n",
    "ax.legend(handles=[patches.Patch(color=color, label=label) for label, color in class_colors.items()], loc='upper right')\n",
    "# Show the image with overlay\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f109d20",
   "metadata": {},
   "source": [
    "### B. Plot of Normalized image vs Annotated Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfec23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "\n",
    "# Load normalized image\n",
    "\n",
    "# Loading annotation data\n",
    "with open(\"annotations/anot.json\", \"r\") as file:\n",
    "    annotation_data = json.load(file)\n",
    "\n",
    "#  a figure and axis with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the normalized image\n",
    "ax1.imshow(normalized_image)\n",
    "ax1.set_title('Normalized Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Display the normalized image with annotations\n",
    "ax2.imshow(normalized_image)\n",
    "\n",
    "# Mapping colors to classes\n",
    "class_colors = {\"Fire\": 'red', \"Smoke\": 'purple', \"No-Fire\": 'green'}\n",
    "\n",
    "# Loop through each bounding box and plot\n",
    "for box in annotation_data[\"boxes\"]:\n",
    "    if box[\"type\"] == \"polygon\":\n",
    "        # Extract polygon points\n",
    "        polygon_points = box[\"points\"]\n",
    "\n",
    "        # Determine class label\n",
    "        class_label = box[\"label\"]\n",
    "\n",
    "        # Get color based on class label\n",
    "        color = class_colors.get(class_label, 'yellow')  # Default to yellow for unknown classes\n",
    "\n",
    "        # Create a polygon patch\n",
    "        polygon = patches.Polygon(polygon_points, linewidth=0.8, edgecolor=color, facecolor='none', label=class_label)\n",
    "\n",
    "        # Add the polygon to the axis\n",
    "        ax2.add_patch(polygon)\n",
    "\n",
    "# Show legend for classes\n",
    "ax2.legend(handles=[patches.Patch(color=color, label=label) for label, color in class_colors.items()], loc='upper right')\n",
    "\n",
    "ax2.set_title('Annotated Image')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7e496",
   "metadata": {},
   "source": [
    "### C. Tile image and Pair Anotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c885a",
   "metadata": {},
   "source": [
    "#### C1. Load Anotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f878a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotation data\n",
    "with open(\"annotations/anot.json\", \"r\") as file:\n",
    "    annotation_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a18ce",
   "metadata": {},
   "source": [
    "#### C2. Tiling Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc508e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_image(normalized_image, tile_size, overlap):\n",
    "    \"\"\"\n",
    "    Creates image tiles with optional overlap, padding incomplete tiles with zeros.\n",
    "\n",
    "    Args:\n",
    "        normalized_image: The normalized image as a NumPy array.\n",
    "        tile_size: A tuple (height, width) specifying the desired tile size.\n",
    "        overlap: A tuple (vertical_overlap, horizontal_overlap) specifying the overlap between tiles.\n",
    "\n",
    "    Returns:\n",
    "        tiled_images: A list of tiled images as NumPy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    image = normalized_image  # Make a copy to avoid modifying the original\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    tile_height, tile_width = tile_size\n",
    "    vert_overlap, horiz_overlap = overlap\n",
    "\n",
    "    # Calculate the number of tiles needed, accounting for potential edge cases\n",
    "    num_vert_tiles = int(np.ceil((height - tile_height) / (tile_height - vert_overlap)) + 1)\n",
    "    num_horiz_tiles = int(np.ceil((width - tile_width) / (tile_width - horiz_overlap)) + 1)\n",
    "\n",
    "    tiles = []\n",
    "    for i in range(num_vert_tiles):\n",
    "        start_y = i * (tile_height - vert_overlap)\n",
    "        end_y = min(start_y + tile_height, height) \n",
    "\n",
    "        for j in range(num_horiz_tiles):\n",
    "            start_x = j * (tile_width - horiz_overlap) \n",
    "            end_x = min(start_x + tile_width, width) \n",
    "\n",
    "            tile = image[start_y:end_y, start_x:end_x]\n",
    "\n",
    "            # Pad if necessary\n",
    "            if tile.shape[0] < tile_height:\n",
    "                pad_y = tile_height - tile.shape[0]\n",
    "                tile = np.pad(tile, ((0, pad_y), (0, 0), (0, 0)), mode='constant') \n",
    "            if tile.shape[1] < tile_width:\n",
    "                pad_x = tile_width - tile.shape[1]\n",
    "                tile = np.pad(tile, ((0, 0), (0, pad_x), (0, 0)), mode='constant')\n",
    "\n",
    "            tiles.append(tile)\n",
    "\n",
    "    return tiles  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates 512x512 tiles with 256 pixel overlap\n",
    "tiles = tile_image(normalized_image.copy(), tile_size=(512, 512), overlap=(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e145d5",
   "metadata": {},
   "source": [
    "#### C3. Print Length of Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93515a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312c7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tile in tiles:\n",
    "    print(tile.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2eca90",
   "metadata": {},
   "source": [
    "#### C4. Print a random tile by specifying the idx number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a random tile\n",
    "\n",
    "plt.imshow(tiles[7])\n",
    "plt.title('Tile')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcf028",
   "metadata": {},
   "source": [
    "#### C5. Plot the first 10 tiles to visualize each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeed726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tiles is a list containing different tiles\n",
    "num_rows = 2\n",
    "num_cols = 5\n",
    "\n",
    "# Create a subplot grid\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 6))\n",
    "\n",
    "# Flatten the 2D array of axes to simplify indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each tile\n",
    "for i in range(num_rows * num_cols):\n",
    "    # Check if there are still tiles left\n",
    "    if i < len(tiles):\n",
    "        axes[i].imshow(tiles[i])\n",
    "        axes[i].set_title(f'Tile {i + 1}')\n",
    "        axes[i].axis('off')\n",
    "    else:\n",
    "        # If no more tiles, remove the empty subplot\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout to prevent clipping of titles\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee7f28-377f-428a-92e6-7a7332ff60f9",
   "metadata": {},
   "source": [
    "##### C5.1 Tile Dimention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94160197-229a-47e7-a950-a3993b224dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tile shape\n",
    "print(f\"Image shape : {tiles[12].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebde996",
   "metadata": {},
   "source": [
    "##### C5.2Visualize Matrix of any random tile, tiles[n] where n = arrIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353d8e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tiles[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotation_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5afa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_and_map_labels(annotation_data, tile_size, overlap, original_image_shape):\n",
    "    \"\"\"\n",
    "    Adjusts annotation labels for image tiles, maps them to a new structure, \n",
    "    and includes the tile index for each tile.\n",
    "\n",
    "    Args:\n",
    "        annotation_data: The original annotation data as a dictionary.\n",
    "        tile_size: A tuple (height, width) specifying the tile size.\n",
    "        overlap: A tuple (vertical_overlap, horizontal_overlap) specifying the overlap between tiles.\n",
    "        original_image_shape: A tuple (height, width) of the original image.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each containing adjusted annotations for a specific tile.\n",
    "    \"\"\"\n",
    "\n",
    "    tile_height, tile_width = tile_size\n",
    "    vert_overlap, horiz_overlap = overlap\n",
    "    orig_height, orig_width = original_image_shape\n",
    "\n",
    "    adjusted_annotations = []\n",
    "\n",
    "    # Calculate tile indices based on original image dimensions\n",
    "    num_vert_tiles = int(np.ceil((orig_height - tile_height) / (tile_height - vert_overlap)) + 1)\n",
    "    num_horiz_tiles = int(np.ceil((orig_width - tile_width) / (tile_width - horiz_overlap)) + 1)\n",
    "\n",
    "    for tile_y in range(num_vert_tiles):\n",
    "        for tile_x in range(num_horiz_tiles):\n",
    "            tile_annotations = {'boxes': []}  # Initialize annotations for this tile\n",
    "\n",
    "            # Calculate tile offsets\n",
    "            offset_x = tile_x * (tile_width - horiz_overlap)\n",
    "            offset_y = tile_y * (tile_height - vert_overlap)\n",
    "\n",
    "            for box in annotation_data['boxes']:\n",
    "                new_box = box.copy()\n",
    "\n",
    "                # Adjust coordinates for tile offset\n",
    "                new_box['x'] = float(box['x']) - offset_x\n",
    "                new_box['y'] = float(box['y']) - offset_y\n",
    "\n",
    "                # Adjust points (only for polygon type)\n",
    "                if box['type'] == 'polygon':\n",
    "                    new_points = []\n",
    "                    for point in box['points']:\n",
    "                        new_points.append([point[0] - offset_x, point[1] - offset_y])\n",
    "                    new_box['points'] = new_points\n",
    "\n",
    "                tile_annotations['boxes'].append(new_box)\n",
    "\n",
    "            # Add tile-specific metadata\n",
    "            tile_annotations['tile_index'] = (tile_y, tile_x)  \n",
    "\n",
    "            adjusted_annotations.append(tile_annotations)\n",
    "\n",
    "    return adjusted_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcca941",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = (512, 512)  # adjusted to match tile size\n",
    "overlap = (0, 0)    # adjusted to match overlap size\n",
    "original_image_shape = (annotation_data['height'], annotation_data['width'])\n",
    "\n",
    "new_annotations = adjust_and_map_labels(annotation_data.copy(), tile_size, overlap, original_image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd83505",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774cb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels_to_integers(new_annotations):\n",
    "    \"\"\"\n",
    "    Maps string labels in annotations to integer codes and converts string-based\n",
    "    'width' and 'height' to numeric types (floats).\n",
    "\n",
    "    Args:\n",
    "        new_annotations: A list of dictionaries where each dictionary represents\n",
    "                         annotations for a tile.\n",
    "\n",
    "    Returns:\n",
    "        The same list of annotation dictionaries with labels mapped to integers \n",
    "    \"\"\"\n",
    "\n",
    "    label_mapping = {\n",
    "        \"Fire\": 1,\n",
    "        \"Smoke\": 2,\n",
    "        \"No-Fire\": 3\n",
    "    }\n",
    "\n",
    "    for tile_annotations in new_annotations:\n",
    "        for box in tile_annotations['boxes']:\n",
    "            box['label'] = label_mapping[box['label']]\n",
    "    return new_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_annotations = map_labels_to_integers(new_annotations.copy())  # Created a copy to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d4d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View mapped annotations\n",
    "mapped_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aedcb6",
   "metadata": {},
   "source": [
    "### D. Training and Validation Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8db1b8",
   "metadata": {},
   "source": [
    "#### D1. Convert labels and tiles to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99740d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert tiles to np array\n",
    "tiles_array = np.stack([np.array(tile) for tile in tiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tiles_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060343ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tiles_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5026000",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a random tile\n",
    "\n",
    "plt.imshow(tiles_array[7])\n",
    "plt.title('Tile')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ebbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tiles_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotations_to_arrays(mapped_annotations):\n",
    "    all_arrays = []\n",
    "    for annotation in mapped_annotations:\n",
    "        points = annotation['boxes'][0]['points']\n",
    "        points_array = np.array(points)\n",
    "        all_arrays.append(points_array)\n",
    "    return np.array(all_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984169c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = convert_annotations_to_arrays(mapped_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d84131",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d18f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of tiles converted to numpy array {tiles_array.shape}')\n",
    "print(f'Shape of labels converted to numpy array {label_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61044b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blank tiles for labelling \n",
    "def create_segmentation_mask(tile, annotations):\n",
    "    mask = np.zeros((512, 512), dtype=np.uint8)  # Blank mask\n",
    "\n",
    "    # Assuming 'annotations' is a dictionary representing a single tile's annotations\n",
    "    for box in annotations['boxes']: \n",
    "        points = np.array(box['points'], dtype=np.int32)\n",
    "        cv2.fillPoly(mask, [points], color=box['label'])  \n",
    "    return mask # Return the created mask\n",
    "\n",
    "# Outside the `create_segmentation_mask` function:\n",
    "label_array = [] \n",
    "for i in range(len(tiles_array)):\n",
    "    tile = tiles_array[i] \n",
    "    tile_annotations = mapped_annotations[i] \n",
    "    mask = create_segmentation_mask(tile, tile_annotations) \n",
    "    label_array.append(mask) # Append the mask for the current tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea66e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label_array to a NumPy array\n",
    "label_array_np = np.array(label_array)\n",
    "\n",
    "# Create an empty array with the desired shape (72, 512, 512, 3)\n",
    "expanded_label_array = np.zeros((label_array_np.shape[0], label_array_np.shape[1], label_array_np.shape[2], 3))\n",
    "\n",
    "# Assign the original label array to the first channel\n",
    "expanded_label_array[:, :, :, 0] = label_array_np\n",
    "\n",
    "# Check the shape of the expanded label array\n",
    "print(f'Expanded label array shape: {expanded_label_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Label array {np.array(expanded_label_array).shape}')\n",
    "print(f'Tiles array {np.array(tiles_array).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08c60e",
   "metadata": {},
   "source": [
    "#### D2. Split Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687824fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b15f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(tiles_array, expanded_label_array, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d8c22",
   "metadata": {},
   "source": [
    "#### D3. Confirm Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "print(f'Type of X_train {type(X_train)}')\n",
    "print(f'Type of X_val {type(X_val)}')\n",
    "print(f'Type of y_train {type(y_train)}')\n",
    "print(f'Type of y_val {type(y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19218e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train {X_train[3]}')\n",
    "print(f'X_val {X_val[3]}')\n",
    "print(f'y_train {y_train[3]}')\n",
    "print(f'y_val {y_val[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00e8f5",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "<p> \n",
    "    - X_train and y_train contains the training data <br> \n",
    "    - X_val and y_val contains the validation data\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02341a3",
   "metadata": {},
   "source": [
    "## 3. UNET Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e91ff",
   "metadata": {},
   "source": [
    "### A. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda456c",
   "metadata": {},
   "source": [
    "### B. SetUp Early Stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory to save the checkpoints\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "\n",
    "# directory check\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'model_checkpoint.h5'),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c9b38",
   "metadata": {},
   "source": [
    "### C. UNET Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet():\n",
    "    inputs = Input((512, 512, 3))\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)  # Reduce filter count\n",
    "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)  # Reduce filter count\n",
    "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)  # Reduce filter count\n",
    "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    # Decoder\n",
    "    up3 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(64, 3, activation='relu', padding='same')(up3)  # Reduce filter count\n",
    "    conv4 = Conv2D(64, 3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    merge1 = concatenate([conv2, conv4], axis=3)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(merge1)  # Reduce filter count\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    conv6 = Conv2D(32, 3, activation='relu', padding='same')(up2)  # Reduce filter count\n",
    "    conv6 = Conv2D(32, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    merge2 = concatenate([conv1, conv6], axis=3)\n",
    "    conv7 = Conv2D(32, 3, activation='relu', padding='same')(merge2)  # Reduce filter count\n",
    "    conv7 = Conv2D(32, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    # Output layer (modify based on your number of classes)\n",
    "    outputs = Conv2D(3, 1, activation='softmax', padding='same')(conv7)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707306e",
   "metadata": {},
   "source": [
    "### D. Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e3c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an appropriate loss function for multiclass segmentation\n",
    "unet_model = unet()\n",
    "unet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f99824",
   "metadata": {},
   "source": [
    "### E. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebbcf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c901788",
   "metadata": {},
   "source": [
    "### F. Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7931561",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "#     callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "#     verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(tiles_array, expanded_label_array, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cb576",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = unet_model.evaluate(test_data, test_labels, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e035f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get first 10 images, view input and ouput images on every line\n",
    "test_preds = unet_model.predict(test_data[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acc6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "print(test_preds.shape)  # Check the shape\n",
    "print(test_preds.dtype)  # Check the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ca517",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = unet_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(images, predictions, class_names):\n",
    "    \"\"\"\n",
    "    Visualizes all test images side-by-side with their dominant fire detection labels.\n",
    "\n",
    "    Args:\n",
    "        images: A NumPy array of the test images.\n",
    "        predictions: A NumPy array of the predicted masks (argmax for class index).\n",
    "        class_names: A list of class names corresponding to the label indices.\n",
    "    \"\"\"\n",
    "    num_images = images.shape[0]\n",
    "    num_rows = int(np.ceil(num_images / 5))  # Adjust columns for better layout\n",
    "    num_cols = min(5, num_images)  # Show max 5 images per row\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "\n",
    "    # Flatten axes for easier iteration\n",
    "    axes_flat = axes.ravel()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        prediction = predictions[i].flatten()\n",
    "\n",
    "        # Check if fire is present based on predicted probabilities\n",
    "        threshold = 0.7  # Adjust the threshold as needed\n",
    "        if np.max(prediction) > threshold:\n",
    "            fire_detected = class_names[np.argmax(prediction)]\n",
    "        else:\n",
    "            fire_detected = \"No Fire Detected\"\n",
    "\n",
    "        class_text = fire_detected\n",
    "\n",
    "        # Plot image and title\n",
    "        axes_flat[i].imshow(image)\n",
    "        axes_flat[i].set_title(class_text)\n",
    "        axes_flat[i].axis('off')\n",
    "\n",
    "    # Hide extra axes if fewer images than total plots\n",
    "    for i in range(num_images, num_rows * num_cols):\n",
    "        axes_flat[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have test_data and test_preds\n",
    "# Get class names (assuming you have them)\n",
    "class_names = [\"Fire\", \"Smoke\", \"No-Fire\"]  # Replace with your actual class names\n",
    "\n",
    "# Visualize all images\n",
    "visualize_predictions(test_data, test_preds.argmax(axis=-1), class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39ce33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
